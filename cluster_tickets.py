"""
https://towardsdatascience.com/clustering-sentence-embeddings-to-identify-intents-in-short-text-48d22d3bf02e
https://umap-learn.readthedocs.io/en/latest/basic_usage.html
https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html
"""
import random
import math
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import umap
import hdbscan
from sentence_transformers import SentenceTransformer
from config import RANDOM_SEED

TRANSFORMER_MODEL = "paraphrase-MiniLM-L6-v2"

np.set_printoptions(precision=4, linewidth=100)

def score_clusters(clusters, prob_threshold, penality_weight):
    """
    Calculate the score for clusters based on various parameters.

    Args:
        clusters (object): The clusters object containing labels and probabilities.
        prob_threshold (float): Penalise fraction of points with proabilities below this threshold
        penality_weight (float): The weight to apply to the unevenness of the cluster sizes.

    Returns:
        tuple: A tuple containing the label count, cost, and sorted label counts.

    """
    labels = clusters.labels_
    label_count = len(np.unique(labels))
    total_num = len(clusters.labels_)
    base_cost = np.count_nonzero(clusters.probabilities_ < prob_threshold) / total_num

    label_counts = {label: len([i for i in labels if i == label]) for label in set(labels)}
    vals = sorted(label_counts.values(), reverse=True)
    penal1, penal2 = 1.0, 1.0
    if len(vals) > 1 and vals[0] != 0:
        penal1 = 1.0 - vals[1]/vals[0]
    if len(vals) > 2 and vals[0] != 0:
        penal2 = 1.0 - vals[2]/vals[0]
    penall = penal1 + 0.1 * penal2

    LEN_CUTOFF = 20
    length_penalty = 1.0 + math.log10(max(label_count, LEN_CUTOFF) / LEN_CUTOFF)

    fail_cost = 1.0 if label_count <= 2 else 0.0

    cost = (base_cost + penality_weight * penall) * length_penalty + fail_cost

    print(f"  **score_clusters:\n\tlabel_count={label_count} " +
          f"cost={cost:.2f} = base_cost={base_cost:.2f} " +
          f"penality_weight={penality_weight} x penall={penall:.2f} x " +
          f"length_penalty={length_penalty:.2f}\n" +
          f"\tvals={vals}")

    return label_count, cost, vals

model = SentenceTransformer(TRANSFORMER_MODEL)

def cluster_labels(clusters):
    """
    Generate a dictionary mapping cluster labels to the indices of sentences belonging to each cluster.

    Args:
        clusters (object): The clusters object containing labels and probabilities.

    Returns:
        dict: A dictionary mapping cluster labels to the indices of sentences belonging to each cluster.

    """
    labels = list(set(clusters.labels_))
    label_cluster = {label: [i for i, l in enumerate(clusters.labels_) if l == label]
                     for label in labels}
    return label_cluster, labels

def generate_clusters(text_embeddings,
                      n_neighbors,
                      n_components,
                      min_cluster_size,
                      random_state,
                      do_plot=True):
    """
    Generates clusters for the given message embeddings using UMAP and HDBSCAN.

    Args:
        text_embeddings (list): List of string embeddings.
        n_neighbors (int): Number of neighbors to consider for UMAP.
        n_components (int): Number of dimensions for the UMAP embeddings.
        min_cluster_size (int): Minimum number of samples required to form a cluster.
        random_state (int): Random seed for reproducibility.

    Returns:
        clusters: The clusters generated by HDBSCAN.

    """
    if do_plot:
        n_components = 2
        n_neighbors = 10
        min_cluster_size = 10

    reducer = umap.UMAP(n_neighbors=n_neighbors,
                        n_components=n_components,
                        metric='cosine',
                        random_state=RANDOM_SEED,
                        )
    clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size,
                                metric='euclidean',
                                gen_min_span_tree=do_plot,
                                cluster_selection_method='eom')

    umap_embeddings = reducer.fit_transform(text_embeddings)
    clusters = clusterer.fit(umap_embeddings)

    if do_plot:
        label_cluster, labels = cluster_labels(clusters)
        # c=[sns.color_palette()[x] for x in penguins.species.map({"Adelie":0, "Chinstrap":1, "Gentoo":2})]
        # palette = sns.color_palette('deep', np.unique(clusters.labels_).max() + 1)
        plt.scatter(
            umap_embeddings[:, 0],
            umap_embeddings[:, 1],
            c=[sns.color_palette('deep', n_components)[x] for x in clusters.labels_],
         )
        plt.gca().set_aspect('equal', 'datalim')
        plt.title('UMAP projection of the summaries dataset', fontsize=24);
        plt.show()

        clusterer.minimum_spanning_tree_.plot(edge_cmap='viridis',
                                      edge_alpha=0.6,
                                      node_size=20,
                                      edge_linewidth=1)
        plt.show()
        clusterer.single_linkage_tree_.plot(cmap='viridis', colorbar=True)
        plt.show()


    return clusters

N_NEIGHBORS = 60
N_COMPONENTS = 4
MIN_CLUSTER_SIZE = 40


N_NEIGHBORS = 50
N_COMPONENTS = 10
MIN_CLUSTER_SIZE = 10

N_NEIGHBORS = 50
N_COMPONENTS = 2
MIN_CLUSTER_SIZE = 10

N_NEIGHBORS = 40
N_COMPONENTS = 10
MIN_CLUSTER_SIZE = 80

N_NEIGHBORS = 60
N_COMPONENTS = 10
MIN_CLUSTER_SIZE = 80

N_NEIGHBORS = 20
N_COMPONENTS = 10
MIN_CLUSTER_SIZE = 20

# n_neighbors=40 n_components=15 min_cluster_size=80
# n_neighbors=40 n_components=15 min_cluster_size=80
# n_neighbors=60 n_components=15 min_cluster_size=80

def find_clusters(sentences,
                  n_neighbors=N_NEIGHBORS,
                  n_components=N_COMPONENTS,
                  min_cluster_size=MIN_CLUSTER_SIZE,
                  tune_clusters=False):
    """
    Finds clusters in `sentences`, a list of short-ish strings.

    Args:
        sentences (list): A list of strings to be clustered.
        n_neighbors (int): The number of neighbors to consider when constructing the k-nearest neighbors graph. Default is N_NEIGHBORS.
        n_components (int): The number of dimensions of the embedding space. Default is N_COMPONENTS.
        min_cluster_size (int): The minimum number of samples required to form a cluster. Default is MIN_CLUSTER_SIZE.
        tune_clusters (bool): If True, the function will tune the cluster parameters using the `find_best_params` function. Default is False.

    Returns:
        tuple: A tuple containing two elements:
            - label_cluster (dict): A dictionary mapping cluster labels to the indices of sentences belonging to each cluster.
            - labels (list): A list of cluster labels, sorted based on the number of sentences in each cluster.

    """
    if tune_clusters:
        n_neighbors, n_components, min_cluster_size = find_best_params(sentences)
        print(f"**find_best_params: n_neighbors={n_neighbors} n_components={n_components} " +
              f"min_cluster_size={min_cluster_size}")
        exit()
    print(f"**find_clusters: sentences={len(sentences)}")
    text_embeddings = model.encode(sentences)
    clusters = generate_clusters(text_embeddings,
            n_neighbors, n_components, min_cluster_size, None)

    return cluster_labels(clusters)

def average(n1: int, n2: int) -> int:
    "Returns the geometric mean of `n1` and `n2` as an integer."
    return int(round(math.sqrt(n1 * n2)))

blend_seed = RANDOM_SEED

def peter_random() -> int:
    "A deterministic random number generator ."
    global blend_seed
    blend_seed = (blend_seed + 1013904223) % 1664525
    return blend_seed % 2**32

def blend(n1: int, n2: int) -> int:
    "Blends `n1` and `n2` using peter_random."
    kind = peter_random() % 3
    if kind == 0:
        return average(n1, n2)
    elif kind == 1:
        return max(n1, n2)
    elif kind == 2:
        return min(n1, n2)
    assert False, f"blend: kind={kind}"

N_CANDIDATES = 20

def blend_best(results, seen, i):
    """
    Blend the best `N_CANDIDATES` results in `results`.

    Args:
        results (list): A list of results.
        seen (set): A set of previously seen selections.
        i (int): The current index.

    Returns:
        tuple: A tuple containing the blended selection of (n_neighbors, n_components, min_cluster_size).
    """
    if len(results) < N_CANDIDATES:
        return None

    sorted_results = sorted(results, key=lambda x: -x[-1])

    for j in range(N_CANDIDATES * 3):
        n = (i + j) % N_CANDIDATES
        assert n < len(sorted_results), (n, len(sorted_results))
        if   n <  5: i1, i2 = 0, 1
        elif n <  8: i1, i2 = 0, 2
        elif n < 10: i1, i2 = 1, 2
        elif n < 11: i1, i2 = 0, 3
        elif n < 12: i1, i2 = 1, 3
        elif n < 13: i1, i2 = 2, 3
        else:        i1, i2 = 0, max(1, n - 13 + 3)

        r1 = sorted_results[i1]
        r2 = sorted_results[i2]
        if   r1[2] == r2[2] and r1[3] == r2[3]:   n_neighbors = average(r1[1], r2[1])
        elif r1[1] == r2[1] and r1[3] == r2[3]: n_components = average(r1[2], r2[2])
        elif r1[1] == r2[1] and r1[2] == r2[2]: min_cluster_size = average(r1[3], r2[3])
        else:
            if r1[1] != r2[1]: n_neighbors = blend(r1[1], r2[1])
            if r1[2] != r2[2]: n_components = blend(r1[2], r2[2])
            if r1[3] != r2[3]: min_cluster_size = blend(r1[3], r2[3])

        selection = (n_neighbors, n_components, min_cluster_size)
        if selection not in seen:
            print(f"@@@@ average: i={i:2}: [{i1},{i2}] selection={selection} r1={r1} r2={r2}")
            return selection

    return None

def blend_best_all(results, seen):
    synthetic_set = set()
    for i in range(3 * N_CANDIDATES):
        synthetic = blend_best(results, seen, i)
        if synthetic and synthetic not in seen:
            synthetic_set.add(synthetic)
    return sorted(synthetic_set)

def random_search(embeddings, space, prob_threshold, penality_weight, num_evals):
    """
    Randomly search hyperparameter space and limited number of times
    and return a summary of the results
    """

    selections = []
    seen = set()
    for i in range(num_evals * 100):
        n_neighbors = random.choice(space['n_neighbors'])
        n_components = random.choice(space['n_components'])
        min_cluster_size = random.choice(space['min_cluster_size'])

        # n_neighbors = min(n_neighbors, len(embeddings)//20)
        # min_cluster_size = min(min_cluster_size, len(embeddings)//10)
        # while n_neighbors + min_cluster_size >= len(embeddings)//10:
        #     if n_neighbors > min_cluster_size:
        #         n_neighbors -= 1
        #     else:
        #         min_cluster_size -= 1

        selection = (n_neighbors, n_components, min_cluster_size)
        if selection in seen:
            continue
        seen.add(selection)
        selections.append(selection)
        if len(selections) >= num_evals:
            break

    print(f"**random_search: selections={len(selections)}")
    for i, (n_neighbors, n_components, min_cluster_size) in enumerate(selections):
        print(f"{i:4}: {n_neighbors:2} {n_components:2} {min_cluster_size:3}")
    results = []

    best_cost = 100.0
    seen = set()

    def do_one(i, selection):
        nonlocal best_cost, results, seen

        (n_neighbors, n_components, min_cluster_size) = selection
        seen.add(selection)
        clusters = generate_clusters(embeddings,
                                     n_neighbors=n_neighbors,
                                     n_components=n_components,
                                     min_cluster_size=min_cluster_size,
                                     random_state=RANDOM_SEED)

        label_count, cost, vals = score_clusters(clusters,
                    prob_threshold=prob_threshold,
                    penality_weight=penality_weight)
        print(f"{i:6}: n_neighbors={n_neighbors} n_components={n_components} min_cluster_size={min_cluster_size}")

        results.append([i, n_neighbors, n_components, min_cluster_size, label_count, cost])

        if cost >= best_cost:
            return

        result_df = pd.DataFrame(results, columns=['run_id', 'n_neighbors', 'n_components',
                                            'min_cluster_size', 'label_count', 'cost'])
        print(result_df.sort_values(by='cost').head(n=20))
        best = result_df.sort_values(by='cost').iloc[0]
        print(f"$$random_search: i={i} label_count={label_count}\n" +
              f"\tbest={best}\n\tvals={vals}")
        best_cost = cost

    for i, selection in enumerate(selections):
        if selection not in seen:
            do_one(i, selection)
        synthetic = blend_best(results, seen, i)
        if synthetic and synthetic not in seen:
            do_one(-i, synthetic)

    for i, synthetic in blend_best_all(results, seen):
        synthetic = blend_best(results, seen, i)
        if synthetic and synthetic not in seen:
            do_one(-(i + len(results)), synthetic)

    result_df = pd.DataFrame(results, columns=['run_id', 'n_neighbors', 'n_components',
                                               'min_cluster_size', 'label_count', 'cost'])

    return result_df.sort_values(by='cost')

def find_best_params(sentences, num_evals=1_000):
    print(f"**find_best_params: sentences={len(sentences)}")
    text_embeddings = model.encode(sentences)
    # n_neighbors=20 n_components=4 min_cluster_size=20
    # n_neighbors=10 n_components=5 min_cluster_size=10
    # n_neighbors=60 n_components=3 min_cluster_size=10
    # n_neighbors=60 n_components=5 min_cluster_size=40
    # n_neighbors=60 n_components=5 min_cluster_size=40
    # n_neighbors=60 n_components=4 min_cluster_size=40 *
    # n_neighbors=10 n_components=3 min_cluster_size=80
    # n_neighbors=10 n_components=3 min_cluster_size=80
    #  60             4                40
    # [1000 rows x 6 columns]
    # run_id              617.00000
    # n_neighbors          40.00000
    # n_components         15.00000
    # min_cluster_size     80.00000
    # label_count           7.00000
    # cost                  0.47966
    # Name: 617, dtype: float64
    # **find_best_params: n_neighbors=40 n_components=15 min_cluster_size=80

    space = {
        'n_neighbors': sorted({N_NEIGHBORS, 10, 20, 40, 60, 100, 200}, reverse=True),
        'n_components': sorted({N_COMPONENTS, 3, 4, 5, 6, 10, 15, 20}, reverse=True),
        'min_cluster_size': sorted({MIN_CLUSTER_SIZE, 20, 40, 60, 80, 120,
                                    # len(sentences)//20, len(sentences)//10, len(sentences)//4
                                    },
                                    reverse=True),
        'random_state': None,
    }
    results = random_search(text_embeddings, space,
                        prob_threshold=0.05,
                        penality_weight=0.1,
                        num_evals=num_evals)
    print(results.head(n=50))
    best = results.iloc[0]
    print(best)
    n_neighbors = int(round(best['n_neighbors']))
    n_components = int(round(best['n_components']))
    min_cluster_size = int(round(best['min_cluster_size']))
    print (f"**find_best_params: n_neighbors={n_neighbors} n_components={n_components} min_cluster_size={min_cluster_size}")
    return n_neighbors, n_components, min_cluster_size
