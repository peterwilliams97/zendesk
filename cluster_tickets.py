"""
https://towardsdatascience.com/clustering-sentence-embeddings-to-identify-intents-in-short-text-48d22d3bf02e
https://umap-learn.readthedocs.io/en/latest/basic_usage.html
https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html
"""
import random
import math
import numpy as np
import pandas as pd
# import matplotlib.pyplot as plt
import umap
import hdbscan
from sentence_transformers import SentenceTransformer
from config import RANDOM_SEED

TRANSFORMER_MODEL = "paraphrase-MiniLM-L6-v2"

np.set_printoptions(precision=4, linewidth=100)

def score_clusters(clusters, prob_threshold, penality_weight):
    """
    Calculate the score for clusters based on various parameters.

    Args:
        clusters (object): The clusters object containing labels and probabilities.
        prob_threshold (float): Penalise fraction of points with proabilities below this threshold
        penality_weight (float): The weight to apply to the unevenness of the cluster sizes.

    Returns:
        tuple: A tuple containing the label count, cost, and sorted label counts.

    """
    cluster_labels = clusters.labels_
    label_count = len(np.unique(cluster_labels))
    total_num = len(clusters.labels_)
    base_cost = np.count_nonzero(clusters.probabilities_ < prob_threshold) / total_num

    label_counts = {label: len([i for i in cluster_labels if i == label]) for label in set(cluster_labels)}
    vals = sorted(label_counts.values(), reverse=True)
    penal1, penal2 = 1.0, 1.0
    if len(vals) > 1 and vals[0] != 0:
        penal1 = 1.0 - vals[1]/vals[0]
    if len(vals) > 2 and vals[0] != 0:
        penal2 = 1.0 - vals[2]/vals[0]
    penall = penal1 + 0.1 * penal2

    LEN_CUTOFF = 20
    length_penalty = 1.0 + math.log10(max(label_count, LEN_CUTOFF) / LEN_CUTOFF)

    fail_cost = 1.0 if label_count <= 2 else 0.0

    cost = (base_cost + penality_weight * penall) * length_penalty + fail_cost

    print(f"  **score_clusters:\n\tlabel_count={label_count} " +
          f"cost={cost:.2f} = base_cost={base_cost:.2f} " +
          f"penality_weight={penality_weight} x penall={penall:.2f} x " +
          f"length_penalty={length_penalty:.2f}\n" +
          f"\tvals={vals}")

    return label_count, cost, vals

model = SentenceTransformer(TRANSFORMER_MODEL)

def generate_clusters(text_embeddings,
                      n_neighbors,
                      n_components,
                      min_cluster_size,
                      random_state):
    """
    Generates clusters for the given message embeddings using UMAP and HDBSCAN.

    Args:
        text_embeddings (list): List of string embeddings.
        n_neighbors (int): Number of neighbors to consider for UMAP.
        n_components (int): Number of dimensions for the UMAP embeddings.
        min_cluster_size (int): Minimum number of samples required to form a cluster.
        random_state (int): Random seed for reproducibility.

    Returns:
        clusters: The clusters generated by HDBSCAN.

    """
    reducer = umap.UMAP(n_neighbors=n_neighbors,
                        n_components=n_components,
                        metric='cosine',
                        random_state=RANDOM_SEED,
                        )
    clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size,
                                metric='euclidean',
                                cluster_selection_method='eom')

    umap_embeddings = reducer.fit_transform(text_embeddings)
    clusters = clusterer.fit(umap_embeddings)

    return clusters

N_NEIGHBORS = 60
N_COMPONENTS = 4
MIN_CLUSTER_SIZE = 40


N_NEIGHBORS = 50
N_COMPONENTS = 10
MIN_CLUSTER_SIZE = 10

N_NEIGHBORS = 50
N_COMPONENTS = 2
MIN_CLUSTER_SIZE = 10

N_NEIGHBORS = 40
N_COMPONENTS = 10
MIN_CLUSTER_SIZE = 80

N_NEIGHBORS = 60
N_COMPONENTS = 10
MIN_CLUSTER_SIZE = 80

#  n_neighbors=40 n_components=15 min_cluster_size=80
# n_neighbors=40 n_components=15 min_cluster_size=80
# n_neighbors=60 n_components=15 min_cluster_size=80

def find_clusters(sentences,
                  n_neighbors=N_NEIGHBORS,
                  n_components=N_COMPONENTS,
                  min_cluster_size=MIN_CLUSTER_SIZE,
                  tune_clusters=False):
    """
    Finds clusters in `sentences`, a list of short-ish strings.

    Args:
        sentences (list): A list of strings to be clustered.
        n_neighbors (int): The number of neighbors to consider when constructing the k-nearest neighbors graph. Default is N_NEIGHBORS.
        n_components (int): The number of dimensions of the embedding space. Default is N_COMPONENTS.
        min_cluster_size (int): The minimum number of samples required to form a cluster. Default is MIN_CLUSTER_SIZE.
        tune_clusters (bool): If True, the function will tune the cluster parameters using the `find_best_params` function. Default is False.

    Returns:
        tuple: A tuple containing two elements:
            - label_cluster (dict): A dictionary mapping cluster labels to the indices of sentences belonging to each cluster.
            - labels (list): A list of cluster labels, sorted based on the number of sentences in each cluster.

    """
    if tune_clusters:
        n_neighbors, n_components, min_cluster_size = find_best_params(sentences)
        print(f"**find_best_params: n_neighbors={n_neighbors} n_components={n_components} " +
              f"min_cluster_size={min_cluster_size}")
        exit()
    print(f"**find_clusters: sentences={len(sentences)}")
    text_embeddings = model.encode(sentences)
    clusters = generate_clusters(text_embeddings,
            n_neighbors, n_components, min_cluster_size, None)
    index_labels = clusters.labels_

    labels = list(set(index_labels))
    label_cluster = {}
    for label in labels:
        label_cluster[label] = [i for i, l in enumerate(index_labels) if l == label]
    labels.sort(key=lambda k: (k < 0, len(label_cluster[k]), k))
    return label_cluster, labels

def average(n1: int, n2: int) -> int:
    "Returns the geometric mean of `n1` and `n2` as an integer."
    return int(round(math.sqrt(n1 * n2)))

blend_seed = RANDOM_SEED

def peter_random() -> int:
    "A deterministic random number generator ."
    global blend_seed
    blend_seed = (blend_seed + 1013904223) % 1664525
    return blend_seed % 2**32

def blend(n1: int, n2: int) -> int:
    "Blends `n1` and `n2` using peter_random."
    kind = peter_random() % 3
    if kind == 0:
        return average(n1, n2)
    elif kind == 1:
        return max(n1, n2)
    elif kind == 2:
        return min(n1, n2)
    assert False, f"blend: kind={kind}"

N_CANDIDATES = 20

def blend_best(results, seen, i):
    """
    Blend the best `N_CANDIDATES` results in `results`.

    Args:
        results (list): A list of results.
        seen (set): A set of previously seen selections.
        i (int): The current index.

    Returns:
        tuple: A tuple containing the blended selection of (n_neighbors, n_components, min_cluster_size).
    """
    if len(results) < N_CANDIDATES:
        return None

    sorted_results = sorted(results, key=lambda x: -x[-1])

    for j in range(N_CANDIDATES * 3):
        n = (i + j) % N_CANDIDATES
        assert n < len(sorted_results), (n, len(sorted_results))
        if   n <  5: i1, i2 = 0, 1
        elif n <  8: i1, i2 = 0, 2
        elif n < 10: i1, i2 = 1, 2
        elif n < 11: i1, i2 = 0, 3
        elif n < 12: i1, i2 = 1, 3
        elif n < 13: i1, i2 = 2, 3
        else:        i1, i2 = 0, max(1, n - 13 + 3)

        r1 = sorted_results[i1]
        r2 = sorted_results[i2]
        n_neighbors = blend(r1[1], r2[1])
        n_components = blend(r1[2], r2[2])
        min_cluster_size = blend(r1[3], r2[3])

        selection = (n_neighbors, n_components, min_cluster_size)
        if selection not in seen:
            print(f"@@@@ average: i={i:2}: [{i1},{i2}] selection={selection} r1={r1} r2={r2}")
            return selection

    return None

def blend_best_all(results, seen):
    synthetic_set = set()
    for i in range(3 * N_CANDIDATES):
        synthetic = blend_best(results, seen, i)
        if synthetic and synthetic not in seen:
            synthetic_set.add(synthetic)
    return sorted(synthetic_set)

def random_search(embeddings, space, prob_threshold, penality_weight, num_evals):
    """
    Randomly search hyperparameter space and limited number of times
    and return a summary of the results
    """

    selections = []
    seen = set()
    for i in range(num_evals * 100):
        n_neighbors = random.choice(space['n_neighbors'])
        n_components = random.choice(space['n_components'])
        min_cluster_size = random.choice(space['min_cluster_size'])

        n_neighbors = min(n_neighbors, len(embeddings)//20)
        min_cluster_size = min(min_cluster_size, len(embeddings)//10)
        while n_neighbors + min_cluster_size >= len(embeddings)//10:
            if n_neighbors > min_cluster_size:
                n_neighbors -= 1
            else:
                min_cluster_size -= 1

        selection = (n_neighbors, n_components, min_cluster_size)
        if selection in seen:
            continue
        seen.add(selection)
        selections.append(selection)
        if len(selections) >= num_evals:
            break

    print(f"**random_search: selections={len(selections)}")
    for i, (n_neighbors, n_components, min_cluster_size) in enumerate(selections):
        print(f"{i:4}: {n_neighbors:2} {n_components:2} {min_cluster_size:3}")
    results = []

    best_cost = 100.0
    seen = set()

    def do_one(i, selection):
        nonlocal best_cost, results, seen

        (n_neighbors, n_components, min_cluster_size) = selection
        seen.add(selection)
        clusters = generate_clusters(embeddings,
                                     n_neighbors=n_neighbors,
                                     n_components=n_components,
                                     min_cluster_size=min_cluster_size,
                                     random_state=RANDOM_SEED)

        label_count, cost, vals = score_clusters(clusters,
                    prob_threshold=prob_threshold,
                    penality_weight=penality_weight)
        print(f"{i:6}: n_neighbors={n_neighbors} n_components={n_components} min_cluster_size={min_cluster_size}")

        results.append([i, n_neighbors, n_components, min_cluster_size, label_count, cost])

        if cost >= best_cost:
            return

        result_df = pd.DataFrame(results, columns=['run_id', 'n_neighbors', 'n_components',
                                            'min_cluster_size', 'label_count', 'cost'])
        print(result_df.sort_values(by='cost').head(n=20))
        best = result_df.sort_values(by='cost').iloc[0]
        print(f"$$random_search: i={i} label_count={label_count}\n" +
              f"\tbest={best}\n\tvals={vals}")
        best_cost = cost

    for i, selection in enumerate(selections):
        if selection not in seen:
            do_one(i, selection)
        synthetic = blend_best(results, seen, i)
        if synthetic and synthetic not in seen:
            do_one(-i, synthetic)

    for i, synthetic in blend_best_all(results, seen):
        synthetic = blend_best(results, seen, i)
        if synthetic and synthetic not in seen:
            do_one(-(i + len(results)), synthetic)

    result_df = pd.DataFrame(results, columns=['run_id', 'n_neighbors', 'n_components',
                                               'min_cluster_size', 'label_count', 'cost'])

    return result_df.sort_values(by='cost')

def find_best_params(sentences, num_evals=1_000):
    print(f"**find_best_params: sentences={len(sentences)}")
    text_embeddings = model.encode(sentences)
    # n_neighbors=20 n_components=4 min_cluster_size=20
    # n_neighbors=10 n_components=5 min_cluster_size=10
    # n_neighbors=60 n_components=3 min_cluster_size=10
    # n_neighbors=60 n_components=5 min_cluster_size=40
    # n_neighbors=60 n_components=5 min_cluster_size=40
    # n_neighbors=60 n_components=4 min_cluster_size=40 *
    # n_neighbors=10 n_components=3 min_cluster_size=80
    # n_neighbors=10 n_components=3 min_cluster_size=80
    #  60             4                40
    # [1000 rows x 6 columns]
    # run_id              617.00000
    # n_neighbors          40.00000
    # n_components         15.00000
    # min_cluster_size     80.00000
    # label_count           7.00000
    # cost                  0.47966
    # Name: 617, dtype: float64
    # **find_best_params: n_neighbors=40 n_components=15 min_cluster_size=80

    space = {
        'n_neighbors': sorted({N_NEIGHBORS, 10, 20, 40, 60, 100, 200}, reverse=True),
        'n_components': sorted({N_COMPONENTS, 3, 4, 5, 6, 10, 15, 20}, reverse=True),
        'min_cluster_size': sorted({MIN_CLUSTER_SIZE, 20, 40, 60, 80, 120,
                                    # len(sentences)//20, len(sentences)//10, len(sentences)//4
                                    },
                                    reverse=True),
        'random_state': None,
    }
    results = random_search(text_embeddings, space,
                        prob_threshold=0.05,
                        penality_weight=0.1,
                        num_evals=num_evals)
    print(results.head(n=50))
    best = results.iloc[0]
    print(best)
    n_neighbors = int(round(best['n_neighbors']))
    n_components = int(round(best['n_components']))
    min_cluster_size = int(round(best['min_cluster_size']))
    print (f"**find_best_params: n_neighbors={n_neighbors} n_components={n_components} min_cluster_size={min_cluster_size}")
    return n_neighbors, n_components, min_cluster_size
